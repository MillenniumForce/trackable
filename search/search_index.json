{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"trackable \u00b6 A minimalistic machine learning model tracker and reporting tool Documentation: https://MillenniumForce.github.io/trackable GitHub: https://github.com/MillenniumForce/trackable PyPI: https://pypi.org/project/trackable/ Free software: MIT trackable is a package focussed on users already familiar with machine learning in Python and aims to: Provide a minimal model tracking tool with no frills An intuitive and lightweight api Installation \u00b6 The latest released version can be installed from PyPI using: # pip pip install trackable Features \u00b6 To start using trackable import the main reporting functionality via: from trackable import Report It's simple to start using the package. The example below (although simplistic) shows how easy it is to pick up the api: from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score , f1_score , roc_auc_score from trackable import Report X , y = make_classification () lr = LogisticRegression () . fit ( X , y ) rf = RandomForestClassifier () . fit ( X , y ) # Instantiate the report... report = Report ( X , y , metrics = [ accuracy_score , f1_score , roc_auc_score ]) # Add models... report . add_model ( lr ) report . add_model ( rf ) # Generate the report... report . generate () Credits \u00b6 This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Home"},{"location":"#trackable","text":"A minimalistic machine learning model tracker and reporting tool Documentation: https://MillenniumForce.github.io/trackable GitHub: https://github.com/MillenniumForce/trackable PyPI: https://pypi.org/project/trackable/ Free software: MIT trackable is a package focussed on users already familiar with machine learning in Python and aims to: Provide a minimal model tracking tool with no frills An intuitive and lightweight api","title":"trackable"},{"location":"#installation","text":"The latest released version can be installed from PyPI using: # pip pip install trackable","title":"Installation"},{"location":"#features","text":"To start using trackable import the main reporting functionality via: from trackable import Report It's simple to start using the package. The example below (although simplistic) shows how easy it is to pick up the api: from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score , f1_score , roc_auc_score from trackable import Report X , y = make_classification () lr = LogisticRegression () . fit ( X , y ) rf = RandomForestClassifier () . fit ( X , y ) # Instantiate the report... report = Report ( X , y , metrics = [ accuracy_score , f1_score , roc_auc_score ]) # Add models... report . add_model ( lr ) report . add_model ( rf ) # Generate the report... report . generate ()","title":"Features"},{"location":"#credits","text":"This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"trackable \u00b6 A minimalistic machine learning reporting tool. To start using trackable, import as, from trackable import Report","title":"Modules"},{"location":"api/#trackable--trackable","text":"A minimalistic machine learning reporting tool. To start using trackable, import as, from trackable import Report","title":"trackable"},{"location":"changelog/","text":"Changelog \u00b6 [0.1.0] - 2022-12-18 \u00b6 Added \u00b6 add_model to add a model to a report generate_report to generate the report Created initial docs Changed \u00b6 None","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#010---2022-12-18","text":"","title":"[0.1.0] - 2022-12-18"},{"location":"changelog/#added","text":"add_model to add a model to a report generate_report to generate the report Created initial docs","title":"Added"},{"location":"changelog/#changed","text":"None","title":"Changed"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/MillenniumForce/trackable/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 trackable could always use more documentation, whether as part of the official trackable docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/MillenniumForce/trackable/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started \u00b6 Ready to contribute? Here's how to set up trackable for local development. Fork the trackable repo on GitHub. Clone your fork locally git clone git@github.com:your_name_here/trackable.git Ensure poetry is installed. Install dependencies and start your virtualenv: poetry install -E test -E doc -E dev Create a branch for local development: git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: poetry run tox Commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check https://github.com/MillenniumForce/trackable/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 poetry run pytest tests/test_trackable.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: poetry run bump2version patch # possible: major / minor / patch git push git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/MillenniumForce/trackable/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"trackable could always use more documentation, whether as part of the official trackable docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/MillenniumForce/trackable/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up trackable for local development. Fork the trackable repo on GitHub. Clone your fork locally git clone git@github.com:your_name_here/trackable.git Ensure poetry is installed. Install dependencies and start your virtualenv: poetry install -E test -E doc -E dev Create a branch for local development: git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: poetry run tox Commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check https://github.com/MillenniumForce/trackable/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"poetry run pytest tests/test_trackable.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: poetry run bump2version patch # possible: major / minor / patch git push git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install trackable, run this command in your terminal: $ pip install trackable This is the preferred method to install trackable, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for trackable can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/MillenniumForce/trackable Or download the tarball : $ curl -OJL https://github.com/MillenniumForce/trackable/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install trackable, run this command in your terminal: $ pip install trackable This is the preferred method to install trackable, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for trackable can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/MillenniumForce/trackable Or download the tarball : $ curl -OJL https://github.com/MillenniumForce/trackable/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 At its core, trackable consists of three elements: Instantiating the Report class Adding models with .add_model(...) Generating the report .generate() To get access to the core functionality use, from trackable import Report In the following sections, assume that all code chunks are in the same python session. Creating a new report \u00b6 To create a report, you need to have defined: Testing data, X_test and y_test A set of metrics, for example accuracy_score from scikit-learn. Below is an example of what this might look like: from sklearn.datasets import make_classification from sklearn.metrics import accuracy_score , f1_score , roc_auc_score report = Report ( X , y , metrics = [ accuracy_score , f1_score , roc_auc_score ]) Adding models \u00b6 All models that are added to the report must contain a .predict method. This is so that the report can calculate and compare metrics. It's easy to add scikit-learn models, however, you may need to write a wrapper for more complex models such as neural networks. For exmaple, let's create a few models and add them to the report: from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier lr = LogisticRegression () . fit ( X , y ) rf = RandomForestClassifier () . fit ( X , y ) report . add_model ( lr ) report . add_model ( rf ) Generating the report \u00b6 To generate the report call the .generate method. By default, the report highlights the model with the maximum value for each metric. However, this can easily be changed to the minimum value, or highlighting can be removed entirely. Finally, to generate the report use: # Turn highlighting off unless you're in a Jupyter notebook report . generate ( highlight = False ) which outputs: accuracy_score f1_score roc_auc_score name LogisticRegression 0.91 0.909091 0.91 RandomForestClassifier 1.00 1.000000 1.00","title":"Usage"},{"location":"usage/#usage","text":"At its core, trackable consists of three elements: Instantiating the Report class Adding models with .add_model(...) Generating the report .generate() To get access to the core functionality use, from trackable import Report In the following sections, assume that all code chunks are in the same python session.","title":"Usage"},{"location":"usage/#creating-a-new-report","text":"To create a report, you need to have defined: Testing data, X_test and y_test A set of metrics, for example accuracy_score from scikit-learn. Below is an example of what this might look like: from sklearn.datasets import make_classification from sklearn.metrics import accuracy_score , f1_score , roc_auc_score report = Report ( X , y , metrics = [ accuracy_score , f1_score , roc_auc_score ])","title":"Creating a new report"},{"location":"usage/#adding-models","text":"All models that are added to the report must contain a .predict method. This is so that the report can calculate and compare metrics. It's easy to add scikit-learn models, however, you may need to write a wrapper for more complex models such as neural networks. For exmaple, let's create a few models and add them to the report: from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier lr = LogisticRegression () . fit ( X , y ) rf = RandomForestClassifier () . fit ( X , y ) report . add_model ( lr ) report . add_model ( rf )","title":"Adding models"},{"location":"usage/#generating-the-report","text":"To generate the report call the .generate method. By default, the report highlights the model with the maximum value for each metric. However, this can easily be changed to the minimum value, or highlighting can be removed entirely. Finally, to generate the report use: # Turn highlighting off unless you're in a Jupyter notebook report . generate ( highlight = False ) which outputs: accuracy_score f1_score roc_auc_score name LogisticRegression 0.91 0.909091 0.91 RandomForestClassifier 1.00 1.000000 1.00","title":"Generating the report"}]}